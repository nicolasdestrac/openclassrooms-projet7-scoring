# ImplÃ©mentez un modÃ¨le de scoring (Projet 7 â€“ OpenClassrooms)

[![Tests](https://img.shields.io/github/actions/workflow/status/nicolasdestrac/openclassrooms-projet7-scoring/ci-cd.yml?label=tests&branch=main)](https://github.com/nicolasdestrac/openclassrooms-projet7-scoring/actions)
[![API Deploy](https://img.shields.io/badge/deploy-render-blue)](#)
[![MLflow](https://img.shields.io/badge/tracking-mlflow-orange)](#)
[![License](https://img.shields.io/badge/license-educational-lightgrey)](#)

Pipeline de scoring de dÃ©faut de crÃ©dit basÃ© sur **Home Credit Default Risk**.
Le projet couvre : ingestion & features, entraÃ®nement **CV**, **mÃ©trique mÃ©tier** (FN â‰« FP) et sÃ©lection de seuil, **tracking MLflow (Databricks)**, **API FastAPI** (Render), **UI Streamlit**, **CI/CD GitHub Actions**, et **monitoring Evidently**.

---

## ğŸ” Vue dâ€™ensemble

- **ModÃ¨les** : Logistic Regression, RandomForest, **LightGBM (final)**
- **PrÃ©process** : imputation mÃ©diane/most_frequent, OHE, normalisation, `log1p` sur colonnes monÃ©taires, features dÃ©rivÃ©es (ratios, interactions `EXT_SOURCE_*`)
- **Validation** : Stratified K-Fold (5 folds), OOF AUC + mÃ©triques par fold
- **Score mÃ©tier** : coÃ»t = `10Ã—FN + 1Ã—FP` â†’ seuil optimal par grille
- **Tracking** : MLflow (Databricks) â€” params, mÃ©triques, artefacts, modÃ¨le
- **Serving** : API FastAPI (Render) + UI Streamlit
- **CI/CD** : tests unitaires (pytest) â†’ dÃ©ploiement Render via **Deploy Hook**
- **Monitoring** : rapport HTML Evidently + alerte JSON, exÃ©cutable en CI

---

## ğŸ“ Arborescence

```
.
â”œâ”€â”€ conf/                 # config YAML (donnÃ©es, CV, coÃ»ts, modÃ¨les, MLflowâ€¦)
â”œâ”€â”€ models/               # artefacts (pipeline.joblib, seuil, schÃ©ma) [gitignored]
â”œâ”€â”€ artifacts/            # rapports data drift [gitignored]
â”œâ”€â”€ reports/              # rapports locaux [gitignored]
â”œâ”€â”€ scripts/              # utilitaires (ex: download Kaggle)
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ data.py           # chargement CSV
â”‚   â”œâ”€â”€ features.py       # feature engineering & prÃ©process
â”‚   â”œâ”€â”€ metrics.py        # mÃ©triques & score mÃ©tier (coÃ»t, seuil optimal)
â”‚   â”œâ”€â”€ train.py          # CV, OOF, logging MLflow, sÃ©rialisation artefacts
â”‚   â”œâ”€â”€ tune.py           # tuning (grid/random) pilotÃ© par conf
â”‚   â”œâ”€â”€ monitor.py        # monitoring Evidently + (optionnel) logging MLflow
â”œâ”€â”€ streamlit_app/
â”‚   â””â”€â”€ app.py            # application Streamlit (front dÃ©mo)
â”œâ”€â”€ api/
â”‚   â””â”€â”€ app.py            # API FastAPI (predict, predict_proba, explain)
â”œâ”€â”€ tests/                # pytest : mÃ©triques, API
â”œâ”€â”€ .github/workflows/
â”‚   â””â”€â”€ ci-cd.yml         # jobs: test | deploy
â”‚   â””â”€â”€ monitor.yml       # job: monitor (drift)
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

---

## ğŸ”§ PrÃ©requis

- Python **3.10**
- Compte **Kaggle** (compÃ©tition *home-credit-default-risk*)
- AccÃ¨s **Databricks** (ou autre backend MLflow)
- (DÃ©mo cloud) Compte **Render** pour lâ€™API et la UI

---

## âš™ï¸ Installation

```bash
git clone https://github.com/nicolasdestrac/openclassrooms-projet7-scoring.git
cd openclassrooms-projet7-scoring

python -m pip install -U pip
pip install -r requirements.txt
```

### Variables dâ€™environnement (`.env`)

```bash
# Kaggle (si tÃ©lÃ©chargement auto)
KAGGLE_USERNAME=...
KAGGLE_KEY=...

# MLflow (Databricks)
MLFLOW_TRACKING_URI=databricks
MLFLOW_EXPERIMENT=/Users/nicolas.destrac@gmail.com/projet7
# DATABRICKS_HOST=https://<workspace>.cloud.databricks.com
# DATABRICKS_TOKEN=<PAT>

# API / CORS (front Streamlit autorisÃ©)
FRONTEND_ORIGINS=https://openclassrooms-projet7-scoring-streamlit.onrender.com
```

Active-les dans la session :

```bash
set -a; source .env; set +a
```

---

## â¬‡ï¸ DonnÃ©es

```bash
# via script
./scripts/download_data.sh

# ou manuellement
kaggle competitions download -c home-credit-default-risk -p data/raw
unzip -o data/raw/home-credit-default-risk.zip -d data/raw
```

> Dossiers volumineux (`data/`, `models/`, `reports/`, `mlruns/`) ignorÃ©s par git.

---

## ğŸ§ª Configuration (extrait `conf/params.yaml`)

```yaml
data:
  train_csv: data/raw/application_train.csv
  test_csv:  data/raw/application_test.csv

cv:
  n_splits: 5
  shuffle: true
  random_state: 42
  early_stopping_rounds: 200

cost:
  fn: 10.0
  fp: 1.0
  threshold_grid: 501

model:
  type: lgbm                  # lgbm | logreg | rf
  lgbm:
    n_estimators: 5000
    learning_rate: 0.03
    num_leaves: 64
    max_depth: 5
    reg_alpha: 0.5
    reg_lambda: 0.1
    subsample: 0.6
    class_weight: balanced
  logreg:
    solver: saga
    max_iter: 2000
    class_weight: balanced
  rf:
    n_estimators: 600
    class_weight: balanced_subsample

mlflow:
  tracking_uri_env: MLFLOW_TRACKING_URI
  default_tracking_uri: databricks
  experiment_env: MLFLOW_EXPERIMENT
  default_experiment: /Users/nicolas.destrac@gmail.com/projet7

artifacts:
  models_dir: models
  reports_dir: reports
```

---

## ğŸƒ EntraÃ®nement & artefacts

```bash
set -a; source .env; set +a
python -m src.train --config conf/params.yaml
```

GÃ©nÃ¨re dans `models/` :

- `scoring_model.joblib` : **Pipeline**(preprocessor, estimator)
- `decision_threshold.json` : `{"threshold": <float>}`
- `input_columns.json` : colonnes dâ€™entrÃ©e attendues (pour lâ€™API)

Et loggue dans **MLflow** :
params, AUC OOF, coÃ»ts/seuils, mÃ©triques par fold, artefacts (importances, matrices), modÃ¨le + signature.

---

## ğŸ”Œ API FastAPI

**Local :**
```bash
uvicorn api.app:app --host 0.0.0.0 --port 8000 --reload
# Docs: http://localhost:8000/docs
```

**Endpoints :**

- `GET /schema` â†’ liste des colonnes dâ€™entrÃ©e
- `POST /predict_proba` â†’ `{"probability": float}`
- `POST /predict` â†’ probabilitÃ© + dÃ©cision binaire via seuil mÃ©tier
- `POST /explain` â†’ top-20 contributions SHAP locales (si compatible)

**Exemple de payload** :
```json
{
  "features": {
    "AMT_CREDIT": 100000.0,
    "AMT_ANNUITY": 12000.0
  }
}
```

---

## ğŸ–¥ï¸ UI Streamlit

```bash
streamlit run streamlit_app/app.py
```

La UI appelle lâ€™API (URL configurable) et affiche probabilitÃ©, dÃ©cision et SHAP local.

---

## âœ… CI/CD GitHub Actions

Fichier : `.github/workflows/ci-cd.yml`

- **test** (PR & `main`)
  - Installe les deps
  - Lance `pytest -q`
- **deploy** (branche `main`, **optionnel**)
  - `needs: test`
  - dÃ©clenche **Render Deploy Hook** si `RENDER_DEPLOY_HOOK` est dÃ©fini
- **monitor** (manuel ou planifiÃ©)
  - exÃ©cute `python -m src.monitor`
  - publie le rapport Evidently en artefact
  - **Ã©choue** le job si `alert.json` indique du drift > seuil

> Sur Render, lâ€™auto-deploy peut Ãªtre **OFF** : le **deploy hook** devient le seul dÃ©clencheur (contrÃ´lÃ© par les tests).

---

## ğŸ“‰ Monitoring (Evidently)

**Local** :
```bash
python -m src.monitor \
  --ref data/raw/application_train.csv \
  --cur data/raw/application_test.csv \
  --out artifacts/reports \
  --sample 50000 \
  --mlflow
```

**Options** :
- `--schema models/input_columns.json` : nâ€™analyser que les features servies
- `--simulate --money-col AMT_CREDIT --money-factor 1.10 --cat-col NAME_INCOME_TYPE --cat-rate 0.25`
- `--drift-share-threshold 0.30` : alerte si > 30% de colonnes driftÃ©es

**Sorties** :
- `evidently_data_drift_report.html`
- `evidently_data_drift_summary.json`
- `alert.json` (clÃ© `alert: true|false` + raison)

---

## ğŸ§ª Tests

```bash
pytest -q
```

- **tests/test_metrics.py** : AUC, coÃ»t mÃ©tier, seuil optimal
- **tests/test_api.py** : `/schema`, `/predict_proba`, `/predict` (happy paths & erreurs)

---

## ğŸ“Š InterprÃ©tabilitÃ©

- **Globale** : importances (gain/impurity pour LGBM) â€” logguÃ©es MLflow
- **Locale** : endpoint `/explain` (SHAP, top-20)

---

## ğŸ” Bonnes pratiques & reproductibilitÃ©

- Seeds fixÃ©s (CV + LGBM)
- SÃ©rialisation **Pipeline sklearn** + signature dâ€™entrÃ©e (MLflow)
- SchÃ©ma contrÃ´lÃ© via `input_columns.json`
- Versions figÃ©es dans `requirements.txt`

---

## ğŸš€ Liens

- **DÃ©pÃ´t GitHub** : https://github.com/nicolasdestrac/openclassrooms-projet7-scoring
- **MLflow (Databricks)** : `/Users/nicolas.destrac@gmail.com/projet7`
- **API Render** : https://openclassrooms-projet7-scoring-api.onrender.com/
- **UI Streamlit** : https://openclassrooms-projet7-scoring-streamlit.onrender.com

---

## ğŸ› ï¸ DÃ©pannage (tips rapides)

- **Databricks** : vÃ©rifier `DATABRICKS_HOST` / `DATABRICKS_TOKEN` pour le tracking MLflow
- **Render** : si auto-deploy OFF, utiliser le **Deploy Hook** (secret GitHub `RENDER_DEPLOY_HOOK`)

---

## ğŸ“œ Licence

Projet pÃ©dagogique â€” usage non commercial.
