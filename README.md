# ImplÃ©mentez un modÃ¨le de scoring (Projet 7 â€“ OpenClassrooms)

Pipeline de scoring de dÃ©faut de crÃ©dit basÃ© sur le dataset **Home Credit Default Risk** (Kaggle).  
Le projet comprend : acquisition des donnÃ©es, feature engineering, entraÃ®nement **CV** avec **LightGBM / Logistic Regression / RandomForest**, mÃ©triques mÃ©tier **FN/FP** et suivi expÃ©rimental **MLflow (Databricks)**.

## ğŸ”§ PrÃ©requis

- Python 3.10
- `pip`
- Compte **Kaggle** (compÃ©tition â€œhome-credit-default-riskâ€ rejointe)
- AccÃ¨s **Databricks** (pour MLflow remote) ou autre backend MLflow si vous adaptez

## ğŸ“¦ Installation

    # Cloner
    git clone git@github.com:nicolasdestrac/openclassrooms-projet7-scoring.git
    cd openclassrooms-projet7-scoring  # (= OpenClassrooms/Projet_7)

    # (optionnel) activer pyenv
    pyenv local scoring_project7
    python -V

    # DÃ©pendances
    pip install -r requirements.txt

## ğŸ” Variables dâ€™environnement

CrÃ©ez un fichier `.env` Ã  la racine :

    # --- Kaggle ---
    KAGGLE_USERNAME=ton_login_kaggle
    KAGGLE_KEY=ta_clef_api_kaggle

    # --- MLflow Remote (Databricks) ---
    MLFLOW_TRACKING_URI=databricks
    MLFLOW_EXPERIMENT=/Users/<ton_email>/projet7_scoring

    # Option A : via variables d'env
    # DATABRICKS_HOST=https://<ton-workspace>.cloud.databricks.com
    # DATABRICKS_TOKEN=<PAT>

    # Option B : via CLI (recommandÃ©)
    # databricks auth login

Chargez-les dans la session :

    set -a; source .env; set +a

## â¬‡ï¸ DonnÃ©es

TÃ©lÃ©charger depuis Kaggle (â‰ˆ700 Mo) :

    ./scripts/download_data.sh
    # => dÃ©pose les CSV dans data/raw/

Ou manuellement :

    kaggle competitions download -c home-credit-default-risk -p data/raw
    unzip -o data/raw/home-credit-default-risk.zip -d data/raw

ğŸ’¡ Les dossiers volumineux (`data/`, `models/`, `mlruns/`, etc.) sont **ignorÃ©s** par git (voir `.gitignore`).

## âš™ï¸ Configuration (conf/params.yaml)

Exemple :

    data:
      train_csv: "data/raw/application_train.csv"
      test_csv:  "data/raw/application_test.csv"

    cv:
      n_splits: 5
      shuffle: true
      random_state: 42
      early_stopping_rounds: 200
      log_period: 50

    cost:               # pondÃ©ration mÃ©tier (FN >>> FP)
      fn: 10.0
      fp: 1.0
      threshold_grid: 501

    model:
      type: "lgbm"      # "lgbm" | "logreg" | "rf"
      lgbm:
        n_estimators: 5000
        learning_rate: 0.03
        num_leaves: 64
        min_child_samples: 100
        subsample: 0.8
        colsample_bytree: 0.8
        reg_lambda: 1.0
        reg_alpha: 0.1
        class_weight: "balanced"
        n_jobs: -1
      logreg:
        penalty: "l2"
        solver: "saga"
        max_iter: 2000
        class_weight: "balanced"
        n_jobs: -1
      rf:
        n_estimators: 500
        max_depth: null
        min_samples_split: 2
        min_samples_leaf: 1
        n_jobs: -1
        class_weight: "balanced"

    mlflow:
      tracking_uri_env: "MLFLOW_TRACKING_URI"
      default_tracking_uri: "databricks"
      experiment_env: "MLFLOW_EXPERIMENT"
      default_experiment: "/Users/<ton_email>/projet7_scoring"

    artifacts:
      models_dir: "models"
      reports_dir: "reports"

**Changer de modÃ¨le** = modifier `model.type` (`lgbm`, `logreg`, `rf`) et ajuster ses hyperparamÃ¨tres.

## ğŸƒâ€â™‚ï¸ EntraÃ®nement

    set -a; source .env; set +a
    python -m src.train --config conf/params.yaml

Pendant le run :
- Cross-validation **StratifiedKFold** (5 folds)
- Logs dÃ©taillÃ©s LightGBM (AUC, early stopping)
- Affichage **AUC par fold** et temps
- Calcul du **seuil optimal** selon les coÃ»ts `FN/FP`
- **MLflow** loggue : AUC OOF, coÃ»t, seuil*, per-fold metrics, artefacts (modÃ¨le, thresholdâ€¦)

En fin de run, la console affiche les liens MLflow (expÃ©rience & run).

## ğŸ“ Structure

    .
    â”œâ”€â”€ conf/
    â”‚   â””â”€â”€ params.yaml
    â”œâ”€â”€ scripts/
    â”‚   â””â”€â”€ download_data.sh
    â”œâ”€â”€ src/
    â”‚   â”œâ”€â”€ data.py            # chargement CSV
    â”‚   â”œâ”€â”€ features.py        # preprocessing / feature eng
    â”‚   â”œâ”€â”€ metrics.py         # AUC, coÃ»t, recherche de seuil
    â”‚   â””â”€â”€ train.py           # pipeline CV + MLflow
    â”œâ”€â”€ data/                  # (ignored) raw/ ...
    â”œâ”€â”€ models/                # (ignored) artefacts locaux
    â”œâ”€â”€ reports/               # (ignored) rapports locaux
    â”œâ”€â”€ requirements.txt
    â””â”€â”€ README.md

## ğŸ“Š MÃ©triques & seuil mÃ©tier

- **AUC OOF** : performance globale CV.  
- **CoÃ»t** : `fn * (#FN) + fp * (#FP)` sur une grille de seuils âˆˆ [0,1].  
- **Seuil*** : seuil qui **minimise** ce coÃ»t.  
Tout est logguÃ© dans MLflow (metrics + params + artefacts).

## ğŸ§ª Suivi expÃ©rimental (MLflow)

Le script configure MLflow pour **Databricks** et gÃ¨re proprement lâ€™ouverture/fermeture des runs :
- `mlflow.set_tracking_uri("databricks")`
- `mlflow.set_experiment("/Users/<email>/projet7_scoring")`
- `mlflow.log_params / log_metrics / log_artifacts`

Comparez facilement les runs et tÃ©lÃ©chargez les artefacts depuis lâ€™UI Databricks.

## ğŸ§¹ Bonnes pratiques Git

- Branches : `feat/<...>`, `fix/<...>`, `chore/<...>`
- Ouvrir une PR vers `main` :

      gh pr create --fill --base main --head feat/ma-feature
      gh pr merge <num> --squash --delete-branch

- Ne jamais committer : `.env`, `data/`, `models/`, `mlruns/`, `mlflow.db`, gros CSV.

## ğŸ› ï¸ DÃ©bogage (FAQ)

- **Kaggle: `Could not find kaggle.json`**  
  â†’ Renseigner `KAGGLE_USERNAME` et `KAGGLE_KEY` dans `.env` **ou** placer `~/.kaggle/kaggle.json` (chmod 600). VÃ©rifier que vous avez **rejoint** la compÃ©tition.

- **MLflow Databricks auth**  
  â†’ `databricks auth login` **ou** exporter `DATABRICKS_HOST` + `DATABRICKS_TOKEN`.

- **LightGBM: â€œDo not support special JSON characters in feature name.â€**  
  â†’ Les features sont renommÃ©es en noms **safe** dans `train.py` (DataFrame + `make_lgbm_safe`). Si vous ajoutez des features, Ã©vitez guillemets/virgules/retours ligne.

- **sklearn ConvergenceWarning (logreg)**  
  â†’ Augmenter `max_iter`, garder `solver="saga"`, veiller au scaling (gÃ©rÃ© dans le preprocessor).

- **â€œRun is already activeâ€ (MLflow)**  
  â†’ Le script ferme maintenant les runs â€œorphelinsâ€. En dernier recours : `mlflow.end_run()`.

## ğŸ“ˆ Roadmap (idÃ©es)

- Hyperparam tuning (Optuna/MLflow)
- API dâ€™infÃ©rence (FastAPI)
- Monitoring data drift (Evidently)
- Dashboard des mÃ©triques

---
