# Données
data:
  train_csv: "data/raw/application_train.csv"
  test_csv:  "data/raw/application_test.csv"

# Split & CV
cv:
  n_splits: 5
  shuffle: true
  random_state: 42
  early_stopping_rounds: 200
  log_period: 100

# Coûts métier (FN >>> FP)
cost:
  fn: 10.0
  fp: 1.0
  threshold_grid: 501   # nb de points entre 0 et 1

# Modèle à entraîner: "lgbm", "logreg" ou "rf"
model:
  type: "lgbm"
  lgbm:
    n_estimators: 5000
    learning_rate: 0.03
    num_leaves: 64
    class_weight: "balanced"

  logreg:
    penalty: "l2"
    solver: "saga"
    max_iter: 2000
    class_weight: "balanced"
    n_jobs: -1

  rf:
    n_estimators: 600
    max_depth: null
    n_jobs: -1
    class_weight: "balanced_subsample"

# MLflow (remote Databricks)
mlflow:
  tracking_uri_env: "MLFLOW_TRACKING_URI"
  default_tracking_uri: "databricks"
  experiment_env: "MLFLOW_EXPERIMENT"
  default_experiment: "/Users/nicolas.destrac@gmail.com/projet7"

# Sorties locales (seront uploadées en artifacts)
artifacts:
  models_dir: "models"
  reports_dir: "reports"

# Tuning
tuning:
  method: "random"        # "grid" ou "random"
  n_iter: 25            # seulement pour method=random
  n_jobs: -1
  cv: 5
  verbose: 2
  refit_metric: "business"

  lgbm:
    param_grid:
      learning_rate: [0.02, 0.03, 0.05]
      num_leaves: [31, 64, 128]
      min_child_samples: [50, 100, 200]
      subsample: [0.8, 1.0]
      colsample_bytree: [0.8, 1.0]
      reg_lambda: [0.0, 1.0]

  rf:
    param_grid:
      n_estimators: [300, 600, 900]
      max_depth: [null, 20, 40]
      min_samples_split: [2, 10]

  logreg:
    param_grid:
      C: [0.1, 0.5, 1.0, 2.0]
      penalty: ["l2"]
      solver: ["saga"]
      max_iter: [2000]
